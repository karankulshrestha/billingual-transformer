
# Attention is all you need - 2017 Transformer Implementation in PyTorch

This is a Billingual Transformer trained on Opus-Books dataset and this is just coded for learning purposes not for any use, but you can use this code for training you this model on your own dataset.




## Enviroment Variables

To run this project, you will need to set up the following environment variables in your environment or in an `.env` file. **Ensure you keep these variables secure and never expose them publicly.**

## Configuration
You'll find the configuration of this model in config.py file.
## Attention Score Matrix

![attention_map](https://github.com/user-attachments/assets/db285592-84e5-4bea-866e-34e75f3ee5a0)


    